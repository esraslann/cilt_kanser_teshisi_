{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DERI KANSERI SINIFLADIRICI MODELI\n",
    "\n",
    "# KÜTÜPHANELER\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#from tensorflow import keras\n",
    "#from keras.applications import imagenet_utils\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import Dense,Dropout, Flatten\n",
    "#from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import glob\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Define Random seed\n",
    "tf.random.set_seed(101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim ve Doğrulama Görüntüleri için Yollar\n",
    "data_dir = 'G:/My Drive/PYTHON WORKS/SKIN-LESION-CLASSIFIER/base_dir/'\n",
    "train_path = data_dir + 'train_dir'\n",
    "valid_path = data_dir + 'val_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(directory):\n",
    "  if not os.path.exists(directory):\n",
    "    return 0\n",
    "  count = 0\n",
    "  for current_path,dirs,files in os.walk(directory):\n",
    "    for dr in dirs:\n",
    "      count += len(glob.glob(os.path.join(current_path,dr+\"/*\")))\n",
    "  return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Girdi görüntüsünün yükseklik, genişlik ve renk ayarı.\n",
    "IMG_CHANNELS = 3\n",
    "IMG_ROWS = 224\n",
    "IMG_COLS = 224\n",
    "input_shape = (IMG_ROWS,IMG_COLS,3)\n",
    "\n",
    "train_samples_num = get_files(train_path)\n",
    "num_classes = len(glob.glob(train_path+\"/*\"))\n",
    "val_samples_num   = get_files(valid_path)\n",
    "#test_samples_num =  len(glob.glob(test_path+\"/*\"))\n",
    "\n",
    "class_labels = ['akiec', 'bcc', 'bkl', 'df', 'mel','nv', 'vasc']\n",
    "\n",
    "# This function will plot images in the form of a grid with 1 row and \n",
    "# 5 columns where images are placed in each column.\n",
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Birkaç yararlı bildirim\n",
    "epochs = 15\n",
    "num_train_samples = train_samples_num \n",
    "num_val_samples = val_samples_num\n",
    "batch_size = 128\n",
    " \n",
    "TARGET_SIZE = (IMG_ROWS, IMG_COLS)\n",
    "OPTIM = tf.keras.optimizers.Adam(lr=0.001)\n",
    "LOSS = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating training and validation set\n",
    "data_gen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Üreticileri ayarlama\n",
    "train_batches = data_gen.flow_from_directory(train_path,\n",
    "                                             target_size=TARGET_SIZE,\n",
    "                                             batch_size=batch_size,\n",
    "                                             color_mode='rgb',\n",
    "                                             class_mode='sparse'\n",
    "                                             )\n",
    "\n",
    "valid_batches = data_gen.flow_from_directory(valid_path,\n",
    "                                             target_size=TARGET_SIZE,\n",
    "                                             batch_size=batch_size,\n",
    "                                             color_mode='rgb',\n",
    "                                             class_mode='sparse'\n",
    "                                             )\n",
    "\n",
    "test_batches = data_gen.flow_from_directory(valid_path,\n",
    "                                            target_size=TARGET_SIZE,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=False,\n",
    "                                            color_mode='rgb',\n",
    "                                            class_mode='sparse'\n",
    "                                            )\n",
    "\n",
    "\n",
    "classes = train_batches.class_indices # dictionary type data classes\n",
    "# Take one image to visualize it's changes after every layer\n",
    "augmented_images = [train_batches[0][0][0] for i in range(5)]\n",
    "plotImages(augmented_images)\n",
    "\n",
    "# Ön-eğitimli imagenet modeli oluşturma\n",
    "#from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.vgg19 import VGG19\n",
    "from keras.applications.densenet import preprocess_input\n",
    "mobile = tf.keras.applications.VGG19(input_shape=input_shape,\n",
    "                     weights='imagenet',\n",
    "                     include_top=False) \n",
    "\n",
    "for layer in mobile.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# imports the model and discards the last layer.\n",
    "# Modeli düzenleme\n",
    "x = mobile.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# layer to convert the features to a single element vector per image.\n",
    "# let's add a fully-connected layer\n",
    "x = Flatten()(x)\n",
    "# Tahminler için dropout ve dense layer ekleme\n",
    "x = Dropout(0.25)(x)\n",
    "#x = Dense(512, activation='relu')(x) \n",
    "# we add dense layers so that the model can learn more complex \n",
    "# functions and classify for better results.\n",
    "x = Dense(100, activation='relu')(x) #dense layer 2\n",
    "#x = Dense(512,activation='relu')(x) #dense layer 3\n",
    "# and a logistic layer -- let's say we have 7 classes\n",
    "# final layer with softmax activation for 7 classes\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Yeni çıktılı modeli oluşturma\n",
    "model = Model(inputs=mobile.input, outputs=predictions)\n",
    "\n",
    "# örnek test görüntüsünü okuma\n",
    "img_path = valid_path + '/bcc/ISIC_0024332.jpg'\n",
    "img = image.load_img(img_path, target_size=TARGET_SIZE)\n",
    "img = image.img_to_array(img)\n",
    "img_test = img/255.\n",
    "plt.imshow(img_test)\n",
    "plt.show()\n",
    "\n",
    "img_test = np.expand_dims(img_test, axis=0)\n",
    "# getting model prediction for unseeen image\n",
    "test_predictions = model.predict(img_test)\n",
    "print(test_predictions)\n",
    "predicted_label = np.argmax(test_predictions)\n",
    "\n",
    "print(class_labels[predicted_label])\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "   print(i, layer.name)\n",
    "# Modeldeki yeni katmanların özeti\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = [layer.name for layer in model.layers]\n",
    "print('layer name : ', model_layers)\n",
    "\n",
    "# Modeli derleme (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer=OPTIM, loss=LOSS, metrics=['accuracy'])\n",
    "\n",
    "# # Save the visualization as a file\n",
    "# tf.keras.utils.plot_model(model, show_shapes=True, \n",
    "#                           to_file=data_dir + 'Transfer_CNN_diagram.png')\n",
    "\n",
    "# Bir iterasyonda kaç adım gerektiğini bildirme\n",
    "train_steps = train_batches.n // train_batches.batch_size\n",
    "val_steps   = valid_batches.n // valid_batches.batch_size\n",
    "\n",
    "# Modeli melanoma daha duyarlı hale getirmek için ağırlık ekleme\n",
    "# class_weights={\n",
    "#     0: 1.0,  # akiec\n",
    "#     1: 1.0,  # bcc\n",
    "#     2: 1.0,  # bkl\n",
    "#     3: 1.0,  # df\n",
    "#     4: 3.0,  # mel\n",
    "#     5: 1.0,  # nv\n",
    "#     6: 1.0,  # vasc\n",
    "# }\n",
    "\n",
    "# Save edilmiş model için dosya yolunu bildirme\n",
    "filepath = data_dir + \"model.h5\"\n",
    "\n",
    "# Modelin en iyi versiyonunu save etmek için checkpoint bildirme\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, \n",
    "                             monitor='val_loss', \n",
    "                             verbose=1,\n",
    "                             save_best_only=True, \n",
    "                             mode='max')\n",
    "\n",
    "# Öğrenme durgunlaştıkça öğrenme oranını azaltma\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', \n",
    "                              factor=0.1, \n",
    "                              patience=2,\n",
    "                              verbose=1, \n",
    "                              mode='max',\n",
    "                              min_delta=1e-4,\n",
    "                              min_lr=1e-4)\n",
    "\n",
    "callbacks_list = [checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli uydurma\n",
    "history = model.fit(train_batches,\n",
    "                    steps_per_epoch=train_steps,\n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim sonuçlarını görselleştirme\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(epochs)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# En iyi dönüm için hesaplama\n",
    "model.load_weights(filepath)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(test_batches, steps=val_steps)\n",
    "\n",
    "print('val_loss:', val_loss)\n",
    "print('val_cat_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test görüntülerinin confusion matrisini oluşturma\n",
    "test_labels = test_batches.classes\n",
    "\n",
    "# Tahmin yapma\n",
    "predictions = model.predict(test_batches,\n",
    "                            test_batches.n//test_batches.batch_size+1,\n",
    "                            verbose=1)\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Confusion matrisi oluşturan fonksiyon\n",
    "def plot_confusion_matrix(cm, classes, normalize=True, \n",
    "                          title='Confusion matris', cmap=plt.cm.Reds):\n",
    "    \"\"\"\n",
    "    Bu fonksiyon, karışıklık matrisini yazdırır ve çizer. \n",
    "    'normalize=True' ile Normalleştirme uygulanabilir.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalize edilmiş karışıklık matrisi\")\n",
    "    else:\n",
    "        print('Normalize edilmemiş Karışıklık matrisi')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('Gerçek Etiket')\n",
    "    plt.xlabel('Tahmini Etiket')\n",
    "    plt.tight_layout()\n",
    "\n",
    "cm = confusion_matrix(test_labels, predictions)\n",
    "\n",
    "plot_confusion_matrix(cm, class_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# örnek test görüntüsünü okuma\n",
    "img_path = valid_path + '/vasc/ISIC_0026876.jpg'\n",
    "img = image.load_img(img_path, target_size=TARGET_SIZE)\n",
    "img = image.img_to_array(img)\n",
    "img_test = np.expand_dims(img/255., axis=0)\n",
    "print(\"image shape: {}\".format(img_test.shape))\n",
    "plt.imshow(img_test)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "img_test = preprocess_input(img_test)\n",
    "# getting model predictions\n",
    "test_predictions = model.predict(img_test)\n",
    "print(test_predictions)\n",
    "predicted_label = np.argmax(test_predictions)\n",
    "\n",
    "print(class_labels[predicted_label])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
